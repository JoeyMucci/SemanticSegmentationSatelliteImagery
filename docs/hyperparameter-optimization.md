# What is BOHB?
For this milestone I implemented hyperparamter optimization into my semantic segmentation using BOHB. BOHB combines the traditional Bayesian Optimization with HyperBand, a bandit-based approach to hyperparameter optimization. Specific details on both will follow below, but the general idea behind combining the two methods is to gain the benefits ofboth methods while having the weaknesses of each method compensated by the other. The result is a relatively fast convergence to optimal hyperparamter configurations, as well as an accurate overall performance. 

# Bayesian Optimization
Bayesian optimization is an improvement over grid search (searching the hyperparameter space at regular intervals) and random search (searching the hyperparameter space at random configurations. It attempts to fit the true objective, or loss, function by taking the data points it gets for the loss calculated at each configuration. Then, instead of choosing configurations randomly, Bayesian Optimization chooses the next configuration by the acquisition function. The acquisition function is maximized when the uncertainty is high and the predicted value of the loss is low. In this way, there is a chance of finding the optimal configuration with the next guess, but either way, the fit of the true objective function will become more accurate by reducing the high variability that was in that area. One strentgh of Bayesian Optimization is that is not tethered to one particular objective function. I was able to simply substitute the loss function from the model and have everything work smoothly. 

# HyperBand
HyperBand is built upon successive halfing, which works like a tournament bracket. A group of configurations are tested, and at regular intervals of the budget (or time, in our case, the number of epochs), the top half of the configurations, i.e. the ones producing a lower value for the objective function, are "winners" and "move on" in the bracket. The method concludes when one configuration (the optimal one) is left and all of the budget is used. One strength of this method is that it converges much faster than random selection, and indeed, this benefit is also observed in the more general BOHB method that I implemented. The way in which HyperBand improves upon the general successive halfing method is by performing the whole procedure multiple times with different budgets. In this way, the impact of the tradeoff between number of configurations searched and number of halvings needed to find the optimal configuration is lessened. 

# Method
I have added two files to this repository for this milestone: HyperparameterOptimization.ipynb and ModelComparison.ipynb. The chief reason behind this is that both notebooks took upwards of an hour to run. I also needed to use a TPU for HyperparamterOptimization. While slower than the GPU, the TPU was more efficient, and actually allowed me to evaluate all the configurations I wanted to without running out of memory. Thus, when actually running the model (for both the standard and optimized parameters) in ModelComparison, I switched back to the faster GPU because memory was no longer an issue. I will describe both notebooks in detail below, but the general idea was to calculate the optimal values for two hyperparameters in HyperparameterOptimization, and then use the values to run the model again and compare the results between using optimized and unoptimized hyperparameters. 

# Hyperparameter Optimization
First I instaleld hpbandster, the hyperband component of BOHB. I then did all the preliminary setup as before, stopping after the model was compiled. Next, I created a worker class that extends from the worker class provided by hpbandster, and manipulated the compute function in two ways. 1) I set the objective function to be the validation loss produced by the model and 2) I added two hyperparameters (batchsize and validation batchsize) to a configuration dictionary. Next, I actually ran BOHB with my worker running in the background to catch any configurations that needed evaluating. BOHB tried several different configurations of batch size and validation batch size and evaluated how well they worked in terms of the objective function I set (the model loss). In this way, my notebook was able to produce the configuration of the batch size and validation batch size that produced the lowest validation loss. 

# Model Comparison
With the ideal hyperparameters calculated, most of the work was done. However, I wanted to observe if the model, when run with the optimized hyperparameter, ran appreciably better or not. I ran the model for 100 epochs twice (once with the hyperparamters original provided by Digital Sreeni and once with the optimized hyperparameters just found), and stored their history in two separate variables. I used the newer model to predict on ten images, but instead of doing random images as in last time, I found the ten images that were predicted the most accurately and displayed them in order of increasing accuracy. The result at first was that images that were basically all one class dominated the top ten. To remedy this issue, I created a function that determined whether or not a image was sufficiently hard to predict by adding the squares of the frequencies of the six classes. After making this change, all the images had significant amoutns at least three classes, so it was shown that the model was accurately predicting on images that were nontrivial to segment. Next, I plotted the validation loss vs # of epochs for the optimized and unoptimized models. Finally, I plotted the precision and recall curves after 20 epochs for both the optimized and unoptimized models.  
