# What is BOHB?
For this milestone I implemented hyperparamter optimization into my semantic segmentation using BOHB. BOHB combines the traditional Bayesian Optimization with HyperBand, a bandit-based approach to hyperparameter optimization. Specific details on both will follow below, but the general idea behind combining the two methods is to gain the benefits ofboth methods while having the weaknesses of each method compensated by the other. The result is a relatively fast convergence to optimal hyperparamter configurations, as well as an accurate overall performance. 

# Bayesian Optimization
Bayesian optimization is an improvement over grid search (searching the hyperparameter space at regular intervals) and random search (searching the hyperparameter space at random configurations. It attempts to fit the true objective, or loss, function by taking the data points it gets for the loss calculated at each configuration. Then, instead of choosing configurations randomly, Bayesian Optimization chooses the next configuration by the acquisition function. The acquisition function is maximized when the uncertainty is high and the predicted value of the loss is low. In this way, there is a chance of finding the optimal configuration with the next guess, but either way, the fit of the true objective function will become more accurate by reducing the high variability that was in that area. One strentgh of Bayesian Optimization is that is not tethered to one particular objective function. I was able to simply substitute the loss function from the model and have everything work smoothly. 

# HyperBand
HyperBand is built upon successive halfing, which works like a tournament bracket. A group of configurations are tested, and at regular intervals of the budget (or time, in our case, the number of epochs), the top half of the configurations, i.e. the ones producing a lower value for the objective function, are "winners" and "move on" in the bracket. The method concludes when one configuration (the optimal one) is left and all of the budget is used. One strength of this method is that it converges much faster than random selection, and indeed, this benefit is also observed in the more general BOHB method that I implemented. The way in which HyperBand improves upon the general successive halfing method is by performing the whole procedure multiple times with different budgets. In this way, the impact of the tradeoff between number of configurations searched and number of halvings needed to find the optimal configuration is lessened. 
