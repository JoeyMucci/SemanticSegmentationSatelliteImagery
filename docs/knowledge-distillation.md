# What is Knowledge Distillation?
Machine learning algorithms are often quite involved and require massive computational power. As such, sometimes it is not feasible to run these algorithms without an incredible amount of resources (Premium GPUs, High-Ram runtimes, etc.). While I was able to access these with Colab Pro+, not everyone will have access to these resources. Furthermore, it is sometimes desirable to have the training handled separately from the usage of a model, because the long wait times that are somewhat tolerable in the training process are unacceptable when a client is trying to make new predictions in real-time. Knowledge distillation is a method to correct the aforementioned issues and allow machine learning algorithms to be of practical use. 

The premise of knowledge distillation is simple: take a cumbersome but robust model that has been thoroughly trained, and convert its information into a smaller model that is much less resource-demanding, while (hopefully) not sacrificing too much accuracy. It takes a softmax of the teacher model at a temperature t, to create what will be called the "soft labels". The higher the temperature, the "softer" the labels are, meaning that the frequency of each of the classes are close. The student model is then put through a softmax of this same temperature t, producing the soft predictions. The soft predictions are then compared with the soft lables via a loss function (e.g. KL divergence). This will be called the distillation loss, because it is a loss between the teacher and student models, as oppposed to the actual hard labels. Speaking of, the student model is also put through a softmax, this time with temperature 1 (low temperature meanns low "softness" remember), and is then compared to the hard labels (same as for teacher model) via a loss function (e.g. Cross Entropy). This is the student loss, because it is the loss of the student model compared to the ground truth. This loss multiplied with α (usually set to 1) is then summed with the product of the distillation loss and a constant, β, to create the general loss that the model will minimize. 
