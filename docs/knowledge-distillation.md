# What is Knowledge Distillation?
Machine learning algorithms are often quite involved and require massive computational power. As such, sometimes it is not feasible to run these algorithms without an incredible amount of resources (Premium GPUs, High-Ram runtimes, etc.). While I was able to access these with Colab Pro+, not everyone will have access to these resources. Furthermore, it is sometimes desirable to have the training handled separately from the usage of a model, because the long wait times that are somewhat tolerable in the training process are unacceptable when a client is trying to make new predictions in real-time. Knowledge distillation is a method to correct the aforementioned issues and allow machine learning algorithms to be of practical use. 

# How does Knowledge Distillation Work?
The premise of knowledge distillation is simple: take a cumbersome but robust model that has been thoroughly trained, and convert its information into a smaller model that is much less resource-demanding, while (hopefully) not sacrificing too much accuracy. It takes a softmax of the teacher model at a temperature t, to create what will be called the "soft labels". The higher the temperature, the "softer" the labels are, meaning that the frequency of each of the classes are close. The student model is then put through a softmax of this same temperature t, producing the soft predictions. The soft predictions are then compared with the soft lables via a loss function (e.g. KL divergence). This will be called the distillation loss, because it is a loss between the teacher and student models, as oppposed to the actual hard labels. Speaking of, the student model is also put through a softmax, this time with temperature 1 (low temperature means low "softness" remember), and is then compared to the hard labels (same as for teacher model) via a loss function (e.g. Cross Entropy). This is the student loss, because it is the loss of the student model compared to the ground truth. This loss multiplied with α (usually set to 1) is then summed with the product of the distillation loss and a constant, β, to create the general loss that the model will minimize. 

# Method
To achieve this milestone I wrote ModelCompression.ipynb, which runs the baseline model, and then implements a knowledge distillation that results in the student model. It then has the student network predict on ten random images, before plotting the training and validation loss vs. number of epochs and precision vs. recall graphs. Importantly, the loss that is graphed is the same dice plus 1focal loss that the original model used. This way, we can see how the student model is performing in general and also compare its performance with the original model because they are both using the same loss metric. 

I did not use NNI's model compression and knowledge distillation resources, because they are implemented with PyTorch and our model is based on the TensorFlow machine learning framework. Instead, I referenced a knowledge distillation article and example on the official keras (which is a part of Tensorflow) machine learning API. The implementation is founded upon the development of a distiller class that extends from the traditional keras models. It overwrites the methods train_step, test_step, and compile. The primary reason why this class is necessary is because there are two models that are involved in the learning process as opposed to just one. One model, the student, is learning from another model, the teacher, whereas usually there is only one model that is doing all the learning by itself. This functionality is achieved by having each model stored in a separate instance variable of the distiller class. Another reason we need the distiller class is so the loss function, which is a combination of the distillation loss and student loss as mentioned above, can be properly handled from within the model. 

I made some modifications to the code in the keras API that made the code more compatible with my model. First, I had to change the loss from SparseCategoricalCrossEntropy to CategoricalCrossEntropy, because the code was intended for a model that had its classes represented as integers (0, 1, 2, etc.) instead of using a one hot encoding, which is what my model uses. I also changed the fromLogits parameter from true to false, because the values that were passed into were coming from softmax, which means they weren't logits. Next I made a change that made the code more in line with my understanding of knowledge distillation concerning the α value. From my reading α had always been set to one, but in this case alpha was a fraction, and (1 - α) multiplied the distillation loss instead of a separate constant, β. So I set α to one and created a new parameter, β, to the compile method that overrides the one from the base keras model. Crucially, I ensured the ratio of β to 1 (the new α value) was equal to the ratio of (1 - α) to α for the old α value. 

With those changes made I just had to do a bit of preparation so that the whole notebook could come together. The code demonstrated making a student model that was a simpler version the teacher model (using Sequential() as an example), so I had to adapt that idea to my model (which is a Functional(), not a Sequential()). To do this I examined Digital Sreeni's simple_multi_unet_model.py to see the exact structure of the model. I then created simple_small_multi_unet_model.py, which created a model of the same format with fewer filters in the Conv2D layers. When initializing the student model, I used a new function get_smaller_model() that fetched the model from simple_small_multi_unet_model.py, instead of using the get_model() that fetches the bigger model from simple_multi_unet_model.py and is called when initializing the teacher model. With the student model initialized, calling distiller.compile() followed by distiller.fit() will begin the knowledge distillation process. Once the student model is done learning, the same figures as in milestone 2 are produced to evaluate the model: ten random images, training and validation loss vs. number of epochs graph, and precision vs. recall graph. Finally, I call summary() on both the teacher model and the student model to show that the student model uses substantially fewer parameters, which demonstrates that the major goal of model compression, creating a smaller model, is achieved. 

# Results
