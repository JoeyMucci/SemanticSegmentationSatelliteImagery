# Modifications
  I made several changes to Sreeni's code in order to achieve the milestone. One of the first changes I had to make was to sort the subdirectories before traversing through them. This ensured that I looped through the images and the corresponding masks in the same order, which was necessary for the data sets to match up. I also changed the metrics that the model is tracking to several PrecisionAtRecall metrics, each with a different base recall value. This change allowed me to be able to graph the precision vs. recall curves for both the training and validation sets. After experimenting with both models in the code, I elected to stick with the first one (based on dice loss and focal loss) because it was faster, and did not perform any worse than the other model. Another modification I made was to recolor the images to the original color scheme after the training was finished. For me, this made it easier to interpret the images because I knew which colors corresponded to which features (land is bright purple, water is orange, etc.). Finally, I implemented a loop so that ten random image predictions were shown instead of just one. 
# Results 
  Generally, the model performed very well. This was pleasantly surprising considering the relatively small size of the data set. All the image predictions closely resembled their corresponding masks. My model does have some trouble with vegetation, in some cases predicting vegetation when there is none and in other cases failing to detect vegetation. This is to be expected, as vegetation does not stand out as much as roads or buildings, and in some cases blends in making it difficult to distinguish from surrounding features. Another area for improvement I noticed was that my model creates predictions that are somewhat more "patchy" then what the ground truth is. The provided masks are well-defined and it is rare that colors overlap with one another. In my predictions, however, I would sometimes notice hints of a certain feature being detected in an area surrounded by another feature. Ideally, in this overlapping scenario the small detections of the other feature would be deemed as insignificant and ignored. 
  My plot of the testing and validation loss vs # of epochs showed that the loss is reduced as more iterations were run, a clear sign that the model is working to accurately predict the images. Around 60 epochs, the validation loss began to stagnate but the training loss continued to decline. This observation means that the model is fitting the test data better but the impact on the validation set is minimal. So, the model is beginning to take into account details that were pertinent to the test data set only and not the general data set. Stopping at 100 epochs ensures that the overfitting problem is never realized, and the validation loss does not begin to increase.
  The precision vs. recall plots showed an inverse relationship. That is, when recall increases, precision decreases and vice versa. This relationship is expected (it was also observed in the logistic regression assignment) and can be explained. A higher recall means that more positives are being detected and by extension the threshold for positive prediction is lower. A lower threshold for positive predictions will cause the rate of positive predictions being correct (AKA precision) to decrease. Similarly, a lower recall will lead to a higher precision. For both the validation and testing sets, my model is achieving >.8 for precision and recall simultaneously, which is a statistical indication (as opposed to the visual one of examining the images) that my model is performing quite well. The training curve was above the validation curve, which is clearly expected since our model is trained using the training set. Therefore, our model will be better at predicting the training data set since it has seen that data before whereas when it predicts on the validation set that is the first time the model is interacting with that data. 
  The baseline performance of the model is quite accurate, yet there is still room for improvement. By performing hyperparameter optimization, I plan to further improve the performance of my model for milestone 3. 
