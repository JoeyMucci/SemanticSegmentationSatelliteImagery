# UNet
  To create the model and achieve this milestone, I made use of UNet, a neural network specifically designed to achieve the task at hand: image segmentation. It works by using a contracting path and an expansive path. The contracting path essentially reduces the dimensions of the image ("going down") and the expansive path stitches those pieces back together after the contracting path is finished ("going up"), resulting in a U-shaped network ("down then up"). UNet makes use of the Rectified Linear Unit (ReLu), which we have seen in class. This function outputs either 0 or the input value, whichever one is higher. Therefore, the gradient calculation is simple (either 0 or 1). This simple gradient calculation makes UNet more efficient than if it had used sigmoid activation, for example. UNet is extremely powerful, and therefore requires substantial computational power to work in a reasonable amount of time. As such, it was necessary for me to upgrade to ColabPro in order to access a powerful GPU which was able to run 100 epochs of my model in a little bit under thirty minutes. 
# Structure 
  The structure of the program (provided by Digital Sreeni) that trains the UNet model is fairly simple. First, the images are accessed and put into smaller patches that will make up the data set. Then, the hexadecimal values are converted to RGB values which are converted into categorical labels ranging from 0-5, representing the features that are being segmented. Next the data set is split into test and train subsets, with the categorical labels representing the ground truth. Two example models are provided which the user can decide between. Either model tracks accuracy and another custom metric, jacard_coef, and runs for 100 epochs. When the model finishes training, training and validation loss vs # of epochs, training and validation IOU (intersection over union) vs # of epochs, and a single image prediction are all plotted. Throughout the program Sreeni provided sanity checks, which served to alert the user of problems that would otherwise not be detected until the program inevitably fails to produce the desired results. 
# Modifications
  I made several changes to Sreeni's code in order to achieve the milestone. One of the first changes I had to make was to sort the subdirectories before traversing through them. This ensured that I looped through the images and the corresponding masks in the same order, which was necessary for the image data set to match up with the ground truth. I also changed the metrics that the model is tracking to several PrecisionAtRecall metrics, each with a different base recall value. This change allowed me to be able to graph the precision vs. recall curves for both the training and validation sets. After experimenting with both models in the code, I elected to stick with the first one (based on dice loss and focal loss) because it was faster, and did not perform any worse than the other model (which used the segmentation models library). Another modification I made was to recolor the images back to the original color scheme after the training was finished. For me, this made it easier to interpret the images because I knew which colors corresponded to which features (land is bright purple, water is orange, etc.). Towards the end of the program, I implemented a loop so that ten random image predictions were shown instead of just one. Finally, I deleted portions of the code that were unnecesary for the milestone. This removal included the IOU graph, the second model, and the sanity checks, but those were useful in helping me realize that my images and masks weren't lining up at first.   
# Results 
  Generally, the model performed very well. This was pleasantly surprising considering the relatively small size of the data set. All the image predictions closely resembled their corresponding masks. My model does have some trouble with vegetation, in some cases predicting vegetation when there is none and in other cases failing to detect vegetation. This is to be expected, as vegetation does not stand out as much as roads or buildings, and in some cases blends in making it difficult to distinguish from surrounding features. Another area for improvement I noticed was that my model creates predictions that are somewhat more "patchy" then what the ground truth is. The provided masks are well-defined and it is rare that colors overlap with one another. In my predictions, however, I would sometimes notice hints of a certain feature being detected in an area surrounded by another feature. Ideally, in this overlapping scenario, the small detections of the other feature would be deemed as insignificant and ignored. 
  
  
  My plot of the testing and validation loss vs # of epochs showed that the loss is reduced as more iterations were run, a clear sign that the model is working to accurately predict the images. Around 60 epochs, the validation loss began to stagnate but the training loss continued to decline. This observation means that the model is fitting the test data better but the impact on the validation set is minimal. So, the model is beginning to take into account details that pertain only to the test data set and not the general data set. Stopping at 100 epochs ensures that the overfitting problem is never realized, and the validation loss does not begin to increase.
  
  
  The precision vs. recall plots showed an inverse relationship. That is, when recall increases, precision decreases and vice versa. This relationship is expected (it was also observed in the logistic regression assignment) and can be explained. A higher recall means that more positives are being detected and by extension the threshold for positive prediction is lower. A lower threshold for positive predictions will cause the rate of positive predictions being correct (AKA precision) to decrease. Similarly, a lower recall will lead to a higher precision. For both the validation and testing sets, my model is achieving >.8 precision and recall simultaneously, which is a statistical indication (as opposed to the visual one of examining the images) that my model is performing quite well. The training curve was above the validation curve, which is clearly expected since our model is trained using the training set. Therefore, our model will be better at predicting the training data set since it has seen that data before whereas when it predicts on the validation set that is the first time the model is interacting with that data. 
  
  
  The baseline performance of the model is quite accurate, yet there is still room for improvement. By performing hyperparameter optimization, I plan to further improve the performance of my model for milestone 3. 
# Figures
![1to3](https://user-images.githubusercontent.com/31972810/200142305-1618c304-4ac9-4cca-b8cd-34c111b05f38.png)
![4to6](https://user-images.githubusercontent.com/31972810/200142307-ff63e5b0-9817-4e93-a271-09bded3680f2.png)
![7to9](https://user-images.githubusercontent.com/31972810/200142370-6dae24b0-e644-424a-a63d-19b6c2ca6766.png)
![10 and graphs](https://user-images.githubusercontent.com/31972810/200142372-279f5f3a-61ca-4220-9e03-448005b75362.png)


