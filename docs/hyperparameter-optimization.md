# What is BOHB?
For this milestone I implemented hyperparameter optimization into my semantic segmentation using BOHB. BOHB combines the traditional Bayesian Optimization with HyperBand, a bandit-based approach to hyperparameter optimization. Specific details on both will follow below, but the general idea behind combining the two methods is to gain the benefits of both methods while having the weaknesses of each method compensated by the other. The result is a relatively fast convergence to optimal hyperparameter configurations, as well as an accurate overall performance. 

# Bayesian Optimization
Bayesian optimization is an improvement over grid search (searching the hyperparameter space at regular intervals) and random search (searching the hyperparameter space at random configurations. It attempts to fit the true objective, or loss, function by taking the data points it gets for the loss calculated at each configuration. Then, instead of choosing configurations randomly, Bayesian Optimization chooses the next configuration by the acquisition function. The acquisition function is maximized when the uncertainty is high and the predicted value of the loss is low. In this way, there is a chance of finding the optimal configuration with the next guess, but either way, the fit of the true objective function will become more accurate by reducing the high variability that was in that area. One strength of Bayesian Optimization is that it is not tethered to one particular objective function. I was able to simply substitute the loss function from the model and have everything work smoothly. 

# HyperBand
HyperBand is built upon successive halving, which works like a tournament bracket. A group of configurations are tested, and at regular intervals of the budget (or time, in our case, the number of epochs), the top half of the configurations, i.e. the ones producing a lower value for the objective function, are "winners" and "move on" in the bracket. The method concludes when one configuration (the optimal one) is left and all of the budget is used. One strength of this method is that it converges much faster than random selection, and indeed, this benefit is also observed in the more general BOHB method that I implemented. The way in which HyperBand improves upon the general successive halving method is by performing the whole procedure multiple times with different budgets. In this way, the impact of the tradeoff between number of configurations searched and number of halvings needed to find the optimal configuration is lessened. 

# Method
I have added two files to this repository for this milestone: HyperparameterOptimization.ipynb and ModelComparison.ipynb. The chief reason behind this is that both notebooks took upwards of an hour to run. I also needed to use a TPU for HyperparameterOptimization. While slower than the GPU, the TPU was more efficient, and actually allowed me to evaluate all the configurations I wanted to without running out of memory. Thus, when actually running the model (for both the standard and optimized parameters) in ModelComparison, I switched back to the faster GPU because memory was no longer an issue. I will describe both notebooks in detail below, but the general idea was to calculate the optimal values for two hyperparameters in HyperparameterOptimization, and then use the values to run the model again and compare the results between using optimized and unoptimized hyperparameters. 

# Hyperparameter Optimization
First I installed hpbandster, the hyperband component of BOHB. I then did all the preliminary setup as before, stopping after the model was compiled. Next, I created a worker class that extends from the worker class provided by hpbandster, and manipulated the compute function in two ways. 1) I set the objective function to be the validation loss produced by the model and 2) I added two hyperparameters (batchsize and validation batchsize) to a configuration dictionary. Next, I actually ran BOHB with my worker running in the background to catch any configurations that needed evaluating. BOHB tried several different configurations of batch size and validation batch size and evaluated how well they worked in terms of the objective function I set (the model loss). In this way, my notebook was able to produce the configuration of the batch size and validation batch size that produced the lowest validation loss. 

# Model Comparison
With the ideal hyperparameters calculated, most of the work was done. However, I wanted to observe if the model, when run with the optimized hyperparameter, ran appreciably better or not. I ran the model for 100 epochs twice (once with the hyperparameters originally provided by Digital Sreeni and once with the optimized hyperparameters just found), and stored their history in two separate variables. I used the newer model to predict on ten images, but instead of doing random images as in last time, I found the ten images that were predicted the most accurately and displayed them in order of increasing accuracy. The result at first was that images that were basically all one class dominated the top ten. To remedy this issue, I created a function that determined whether or not a image was sufficiently hard to predict by adding the squares of the frequencies of the six classes. After making this change, all the images had significant amounts at least three classes, so it was shown that the model was accurately predicting on images that were nontrivial to segment. Next, I plotted the validation loss vs # of epochs for the optimized and unoptimized models. Finally, I plotted the precision and recall curves after 20 epochs for both the optimized and unoptimized models.  

# Results
The result of running the hyperparameter optimization was a batch size of 2 and a validation batch size of 29. So, a lower batch size for the training data and a higher batch size for the validation batch size will decrease the validation loss. Digital Sreeni elected to set both batch sizes to 16 when he ran the model by only specifying one batch size. He likely tried a few different values for the batch size and determined 16 was around the best result if both batch sizes were to be the same. This makes sense, because the average of the two batch sizes I obtained via hyperparameter optimization was just about 16. However, using BOHB and separating the batch size from the validation batch size allowed me to observe that the optimal values for the batch sizes were actually quite different from what Digital Sreeni provided, and changing them can lead my model to significantly enhance its performance. 


The new model predicted on all the images and then displayed the 10 most accurate images that were sufficiently difficult to predict. The accuracy ranged from 85% to 90%, which is a satisfactory result especially considering the images were nontrivial. Examining them visually, I observe that the model is detecting almost all the features, and it is just a little bit rough around the edges. Although not massive, it is even possible to observe improvements in the image segmentation with this new model compared to the unoptimized model by the eye test, an example of which I will provide below. 


The plot of validation loss vs. number of epochs clearly shows that the model with optimized hyperparameters is performing better, as the validation is considerably lower for the optimized model until about 40 epochs. After that point, the models appear to be performing about the same. The plot of precision and recall at 20 epochs shows that the optimized model is making fewer mistakes, as the optimized model’s precision values are higher than the unoptimized model’s precision values at every recall value. The same general trends from the baseline performance (loss stagnating at about 60 epochs, precision and recall inverse relationship, etc.) were also observed after hyperparameter optimization. 


It is obvious that hyperparameter optimization improved the overall performance of the model. The benefit was seen chiefly in the form of faster convergence, rather than the actual loss being reduced. However, speed of convergence is of practical concern to us. Considering that the code for this milestone took about three hours to run, and the fact that this data set is relatively small as machine learning datasets are concerned, fast converging methods are needed to be able to feasibly obtain results in the general case. 


That is why for the fourth milestone I will be investigating model compression. Combined with the faster convergence that hyperparameter optimization gave us, model compression can greatly reduce the time needed to run my model. A compressed version of my model would be more widely applicable to be used by others who may not have access to the computing resources that I have at my disposal with Colab Pro (GPUs and TPUs). And quite excitingly, my model may be able to tackle even more challenging tasks with much larger data sets. 

# Figures
![optimizedhyperparameters](https://user-images.githubusercontent.com/31972810/202926771-508234d2-d95c-47e6-b8d6-8423293d2be9.jpg)
![image_comparison](https://user-images.githubusercontent.com/31972810/202926577-bd4452ab-7db2-40ea-bc30-9260cd0b5350.png)
![10-8](https://user-images.githubusercontent.com/31972810/202926031-75b56a39-6548-46aa-8488-d42c8f17d298.jpg)
![7-5](https://user-images.githubusercontent.com/31972810/202926043-9528517a-9542-48b9-b68e-8d245571ce96.jpg)
![4-2](https://user-images.githubusercontent.com/31972810/202926045-3fcd6f74-fe4e-4f8b-83bb-f81485c2bb37.jpg)
![1 and graphs](https://user-images.githubusercontent.com/31972810/202926046-a53d1e00-d370-45f5-aa59-98931d57d948.jpg)

